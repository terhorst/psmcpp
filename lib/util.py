import numpy as np
import functools
import itertools
import multiprocessing

sawtooth = {
        'a': np.array([7.1, 7.1, 0.9, 7.1, 0.9, 7.1, 0.9]),
        'b': np.array([7.1, 0.9, 7.1, 0.9, 7.1, 0.9, 0.9]),
        's_gen': np.array([1000.0, 4000.0 - 1000., 10500. - 4000., 65000. - 10500., 115000. - 65000., 1e6 - 115000, 1.0]) / 25.0
        }

humany = {
    'a': np.array([10.0, 0.5, 1.0, 2.0, 3.0]),
    'b': np.array([1.0, 0.5, 1.0, 2.0, 3.0]),
    's_gen': np.array([10000., 70000. - 10000., 150000. - 70000., 1.0]) / 25.0
    }

def grouper(iterable, n, fillvalue=None):
    "Collect data into fixed-length chunks or blocks"
    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx
    args = [iter(iterable)] * n
    return itertools.izip_longest(fillvalue=fillvalue, *args)

def unpack(iterable):
    for span, x in iterable:
        for i in range(span):
            yield x

def pack(seq):
    iterable = iter(seq)
    x = next(iterable)
    i = 1
    for xp in iterable:
        if xp == x:
            i += 1
        else:
            yield (i, x)
            x = xp
            i = 1
    yield (i, x)

def memoize(obj):
    cache = obj.cache = {}
    @functools.wraps(obj)
    def memoizer(*args, **kwargs):
        key = tuple(args) + tuple(kwargs.items())
        if key not in cache:
            cache[key] = obj(*args, **kwargs)
        return cache[key]
    return memoizer

def kl(sfs1, sfs2):
    s1 = sfs1.flatten()
    s2 = sfs2.flatten()
    nz = s1 != 0.0
    return (s1[nz] * (np.log(s1[nz]) - np.log(s2[nz]))).sum()

def dataset_from_panel(dataset, n, distinguished_rows, random=True):
    L, positions, haps = dataset[:3]
    dr = list(distinguished_rows)
    K = haps.shape[1]
    if n < haps.shape[0]:
        panel = haps[[i for i in range(haps.shape[0]) if i not in dr]]
        N, K = panel.shape
        h2 = np.zeros([n, K], dtype=np.int8)
        h2[:2] = haps[dr]
        for i in range(K):
            inds = [j for j in range(N) if j not in dr and panel[j, i] != -1]
            if random:
                inds = np.random.permutation(inds)
            assert len(inds) >= n - 2
            h2[2:, i] = panel[:, i][inds[:(n - 2)]]
        haps = h2
    else:
        perm = np.arange(n)
        perm[[0, 1] + dr] = perm[dr + [0, 1]]
        haps = haps[np.ix_(perm, np.arange(K))]
    seg = np.logical_and(*[(haps != a).sum(axis=0) > 0 for a in [0, 1]])
    return (L, positions[seg], haps[:, seg]) + dataset[3:]

def hmm_data_format(dataset, n, distinguished_rows, missing=0.):
    # Convert a dataset generated by simulate() to the format accepted
    # by the inference code
    ret = []
    p = 0
    L, positions, haps = dataset_from_panel(dataset, n, distinguished_rows)[:3]
    if missing > 0:
        haps[np.random.random(size=haps.shape) < missing] = -1
    d = haps[:2].sum(axis=0)
    d[haps[:2].min(axis=0) == -1] = -1
    t = np.maximum(0, haps[2:]).sum(axis=0)
    en = (haps[2:] != -1).sum(axis=0)
    nd = d.shape[0]
    nrow = 2 * nd - 1
    ret = np.zeros([nrow, 4], dtype=int)
    ret[::2, 0] = 1
    ret[::2, 1] = d
    ret[::2, 2] = t
    ret[::2, 3] = en
    gaps = positions[1:] - positions[:-1] - 1
    ret[1::2, 0] = gaps
    ret[1::2, 1:3] = 0
    ret[1::2, 3] = n - 2
    if positions[0] > 0:
        ret = np.vstack(([positions[0], 0, 0, n - 2], ret))
    if positions[-1] < L - 1:
        ret = np.vstack((ret, [L - 1 - positions[-1], 0, 0, n - 2]))
    # eliminate "no gaps"
    ret = ret[ret[:, 0] > 0]
    # assert np.all(ret >= 0)
    assert ret.sum(axis=0)[0] == L, (L, ret.sum(axis=0)[0], ret)
    ret = np.array(ret, dtype=np.int32)
    assert ret.sum(axis=0)[0] == L
    assert np.all(ret[:, 0] >= 1)
    return ret

def normalize_dataset(A, thinning):
    '''Normalize list of observations for inputting into the model.
    Namely, make sure the span of the first row is 0 and implement the 
    thinning procedure needed to break up correlation among the full
    SFS emissions.'''
    if A[0, 0] > 1:
        np.insert(A, 0, [1] + list(A[0, 1:]), 0)
        A[1, 0] -= 1
    # Thinning
    i = 0
    out = []
    for span, a, b, nb in A:
        a1 = np.sign(a) * (a % 2)
        while span > 0:
            if i < thinning and i + span >= thinning:
                out.append([thinning - i - 1, a1, 0, 0])
                out.append([1, a, b, nb])
                span -= thinning - i
                i = 0
            else:
                out.append([span, a1, 0, 0])
                i += span
                break
    ret = []
    lastobs = out[0]
    for obs in out[1:]:
        if obs[0] == 0:
            continue
        if obs == lastobs:
            lastobs[0] += obs[0]
        else:
            ret.append(lastobs)
            lastobs = obs
    ret.append(lastobs)
    if ret[0][0] > 1:
        ret.insert(0, [1] + ret[0][1:])
        ret[1][0] -= 1
    ret = np.array(ret, dtype=np.int32)
    assert ret[:, 0].sum() == A[:, 0].sum()
    return ret

def _pt_helper(fn):
    A = np.loadtxt(fn, dtype=np.int32)
    A[np.logical_and(A[:,1] == 2, A[:,2] == A[:,3]), 1:3] = 0
    return A

def parse_text_datasets(datasets):
    p = multiprocessing.Pool(None)
    obs = list(p.map(_pt_helper, datasets))
    p.close()
    p.terminate()
    p = None
    n = 2 + max([ob[:,-1].max() for ob in obs])
    return {'n': n, 'obs': obs}

def config2dict(cp):
    return {sec: dict(cp.items(sec)) for sec in cp.sections()}
